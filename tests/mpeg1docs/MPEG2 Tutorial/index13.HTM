<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>
<head>

	<META NAME="VPSiteProject" CONTENT="file:///C|/Bretl%20Site%20copy/BretlSite.vpp"><meta name="generator" content = "HyperText Studio from Olson Software (http://www.olsonsoft.com)">
<meta name="formatter" content = "HyperText Studio from Olson Software (http://www.olsonsoft.com)">
<title>Lossy Coding</title>
<style><!-- body { }
H5{}
H2{}
H3{}
H6{}
H4{}
H1{}
P{}
.DefaultParagraphFont{}
 --></style>
</head>
<body>
<BANNER>

<p><font size="5" face="Arial"><span class="DefaultParagraphFont" style="font-size: 16pt;"><b>Lossy Coding</b></span></font><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;"></span></font></p></BANNER>

<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">For lossless coding to be successful, it must be presented with source material which generally fits the statistics for which it was designed. &nbsp;In most practical cases of audio or video coding, this means a source in which low-frequency content dominates. &nbsp;</span></font></p>
<p>&nbsp;</p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">Because the statistics of real sources may not be constant (for example, music may contain cymbal crashes, or pictures may contain high contrast texture or text), the bit rate of the coder output will generally vary. &nbsp;Buffering between the coder output and the transmission or storage medium is helpful to absorb peaks in bit rate. &nbsp;However, it is always possible to &quot;break&quot; the system by presenting it with source material that does not match the preferred statistics. &nbsp;In a system designed to be lossless, there may be a need to handle these extreme cases with a controlled failure mode, that is, by controlling what bits are discarded; but the design goal is to avoid this crash for the great majority of source material.</span></font></p>
<p>&nbsp;</p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">Lossy coding assumes that some information can always be discarded. &nbsp;This results in a controlled degradation of the decoded signal; instead of crashing, the system is designed to gradually degrade as less and less bit rate is available for transmission (or as more and more is required for difficult sources). &nbsp;The goal is a reproduction that is visually or aurally indistinguishable from the source, or failing that, one that has artifacts of the least possible objectionability. </span></font></p>
<p>&nbsp;</p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">Lossy coding of pictures could be based on completely eliminating some </span></font><a href="index100.HTM" target="popup"><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">DCT</span></font></a><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;"> coefficients, but it has been found to be better to adjust the quantizing coarseness of the coefficients, with the extreme case being a quantizing step so big that the particular coefficient is quantized to zero.</span></font></p>
<p>&nbsp;</p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">The design of the system to discard or coarsely quantize data depends on several factors, a few of which are:</span></font></p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">1) what form of the data is most efficient to calculate (e.g., DCT coefficients);</span></font></p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">2) how much energy is concentrated in particular coefficients (so others are likely to be unnoticed if discarded, because they contain only small signal energy);</span></font></p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">3) what is the relative visibility/audibility of discarding various data/coefficients of equal energy;</span></font></p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">4) is the visibility/audibility of artifacts strongly affected by joint spectral/spatial/temporal effects, or can data discard be based only on transform coefficients;</span></font></p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">5) if joint effects are a problem, do they need to be explicitly considered, or is there a method based on the transform coefficients that takes the other factors (at least partly) into account by default;</span></font></p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">6) in the chosen design(s), what is the final result of artifact levels vs. data rate reduction.</span></font></p>
<p>&nbsp;</p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">Item number 6 above is referred to as &quot;rate-distortion theory&quot;. &nbsp;It can usually be studied mathematically only by using statistical models of the system that result in predictions of root-mean-square (rms) coding noise (difference between the original and decoded signals - </span></font><a href="index99.HTM" target="popup"><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">coding error</span></font></a><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;"> ) as a function of bits/pel (or bits/audio sample) and entropy of the source. &nbsp;In computer simulations of a system, rate/distortion curves can be measured directly. In this case, peak signal to noise ratio </span></font><a name="Anchor1" ></a><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">(</span></font><a href="index121.HTM" target="popup"><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">PSNR</span></font></a><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">) is often used, as it may have slightly more relation to visibility. </span></font></p>
<p>&nbsp;</p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">While rms signal to noise ratio or </span></font><a href="index121.HTM" target="popup"><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">PSNR</span></font></a><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;"> can give a reasonable comparison of the performance of a particular system with some varied parameters, these criteria fail in the comparison of different systems, because the visibility/audibility of different </span></font><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;"><i>kinds </i></span></font><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;"></span></font><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">of artifacts will generally be different even though their PSNR or rms signal-to-noise ratio is the same. &nbsp;This is because of the interaction of spatial, temporal and frequency effects as speculated in item number 4 above. &nbsp;In actuality, the human visual and auditory systems have many successive layers of processing as well as some parallel processing functions at particular layers. &nbsp;The task of designing a lossy coding system that produces undetectable errors in all these processes with minimum transmitted bits is difficult and nowhere near completely solved. Note that even in this case we are only trying to make a good reproduction of a video image, which in its best original condition contains enough artifacts to be instantly distinguishable from a real-world scene.</span></font></p>
<p>&nbsp;</p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">The result is that it is extremely difficult to develop an objective measure of lossy coder performance. &nbsp;The only reliable method available to date requires subjective viewing/auditioning under carefully controlled repeatable conditions, with a large group of viewers/auditors. &nbsp;Some early methods/hardware were developed that gave results reasonably correlated to subjective tests when confined to a particular system under non-extreme conditions. &nbsp;</span></font></p>
<p>&nbsp;</p>
<p><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">Commercial image quality measurement hardware has been introduced based on visual models of luminance, temporal effects and full color effects, which outputs results in terms of </span></font><a href="index134.HTM" target="popup"><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">JND</span></font></a><font size="3" face="Arial"><span class="DefaultParagraphFont" style="font-size: 10pt;">'s between an uncoded reference picture and the encoded/decoded version.  These units are useful especially if there is a need to contractually guarantee a measured quality of service.</span></font></p></body>
</html>
